accumulate_grad_batches: 16
batch_size: 8
config_path: ./pretrained/small_117M/config.json
dataset: dialo_dataset
default_root_dir: checkpoints
distributed_backend: dp
fast_dev_run: false
find_unused_parameters: false
gpus: '0'
gradient_clip_val: 0.5
is_test: false
load_best: false
load_dir: checkpoints
load_v_num: 3
load_ver: version
log_dir: lightning_logs
loss: ce
lr: 0.001
lr_decay_min_lr: 1.0e-05
lr_decay_rate: 0.5
lr_decay_steps: 20
lr_scheduler: null
max_epochs: 100
max_length: 512
min_epochs: 5
model_name: GPT2
no_augment: false
num_workers: 8
optimizer: null
precision: 16
pretrained: false
pretrained_generator_path: null
pretrained_selector_path: null
seed: 1104
test_data_dir: ref/test.txt
train_data_dir: ref/train.txt
val_check_interval: 100000
valid_data_dir: ref/dev.txt
vocab_path: ./pretrained/small_117M/vocab.json
warm_up_steps: 4000
weight_decay: 1.0e-05
word_embeddings: null
